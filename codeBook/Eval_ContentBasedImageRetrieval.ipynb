{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eval.ContentBasedImageRetrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaiNga-uit/CS2224.CH1501/blob/main/codeBook/Eval_ContentBasedImageRetrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8bXhNJVx2gL"
      },
      "source": [
        "# Evaluation\r\n",
        "* This step uses the groundtruth from Oxford to calculate mean average precision of relevant queries\r\n",
        "* The precision is calculated at top 1, 4, 10 and 100 returned images. \r\n",
        "\r\n",
        "* Note: The model for 5k images were extracted before and upload to drive. Please refer to the following codebook for more information \r\n",
        "https://drive.google.com/file/d/1gl66ZCZ9zVT615hrMcoV3eaE5mweQR2i/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZdCXgYgyXqf"
      },
      "source": [
        "## Declare and create some folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on-kMrply_Bs"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "def MakeDirWithChecked(path):\r\n",
        "    if not os.path.isdir(path):\r\n",
        "        os.mkdir(path)\r\n",
        "\r\n",
        "evalDir = \"/content/eval\"\r\n",
        "evalGroundTruth = \"/content/eval/ground_truth\"\r\n",
        "evalQueryImg = \"/content/eval/query_img\"\r\n",
        "evalRankList = \"/content/eval/rank_list\"\r\n",
        "\r\n",
        "datasetDir = \"/content/dataset/\"\r\n",
        "databaseDir = \"/content/database\"\r\n",
        "\r\n",
        "MakeDirWithChecked(evalDir)\r\n",
        "MakeDirWithChecked(evalGroundTruth)\r\n",
        "MakeDirWithChecked(evalQueryImg)\r\n",
        "MakeDirWithChecked(evalRankList)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aslvn8gm2aTE"
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "from typing import List\r\n",
        "\r\n",
        "def load_list(fname: str):\r\n",
        "    \"\"\"Plain text list loader. Reads from file separated by newlines, and returns a\r\n",
        "    list of the file with whitespaces stripped.\r\n",
        "    Args:\r\n",
        "        fname (str): Name of file to be read.\r\n",
        "    Returns:\r\n",
        "        List[str]: A stripped list of strings, using newlines as a seperator from file.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    return [e.strip() for e in open(fname, 'r').readlines()]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe0su-5Kyah7"
      },
      "source": [
        "## Prepare query image\r\n",
        "Prepare query image relevant to 55 queries in groundtruth and write to evalQueryImg = \"/content/eval/query_img\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ligB2POeLQ44"
      },
      "source": [
        "# Don't want to crop query img?\r\n",
        "# Just download prepared query img\r\n",
        "# Enabled these below line and run then move directly to run classifying step\r\n",
        "\r\n",
        "#import gdown\r\n",
        "\r\n",
        "modelUrl = 'https://drive.google.com/uc?id=1hUff66nwWyhBf1UG-UrcngYHPSXgcGUs' #URL cố định dùng để download.\r\n",
        "output = '/content/eval/query_img/query_img.zip' \r\n",
        "gdown.download(modelUrl, output, quiet=False)\r\n",
        "\r\n",
        "!unzip -o '/content/eval/query_img/query_img.zip' -d '/content/eval'\r\n",
        "!rm -r '/content/eval/query_img/query_img.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOmIxemX0l1V"
      },
      "source": [
        "import os, sys, tarfile\r\n",
        "\r\n",
        "def extract(tar_url, extract_path='.'):\r\n",
        "    # print tar_url\r\n",
        "    tar = tarfile.open(tar_url, 'r')\r\n",
        "    for item in tar:\r\n",
        "        tar.extract(item, extract_path)\r\n",
        "        if item.name.find(\".tgz\") != -1 or item.name.find(\".tar\") != -1:\r\n",
        "            extract(item.name, \"./\" + item.name[:item.name.rfind('/')])\r\n",
        "try:\r\n",
        "\r\n",
        "    extract(sys.argv[1] + '.tgz')\r\n",
        "    # print 'Done.'\r\n",
        "except:\r\n",
        "    name = os.path.basename(sys.argv[0])\r\n",
        "    # print name[:name.rfind('.')], '<filename>'"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KktoAB90rAW"
      },
      "source": [
        "#TODO: upload groundtruth to evalGroundTruth = \"/content/eval/ground_truth\"\r\n",
        "import gdown\r\n",
        "datasetUrl = 'https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/gt_files_170407.tgz'\r\n",
        "downloadFolder = '/content/eval/ground_truth/groundtruth.tgz' \r\n",
        "gdown.download(datasetUrl, downloadFolder, quiet=False)\r\n",
        "\r\n",
        "extract('/content/eval/ground_truth/groundtruth.tgz', evalGroundTruth)\r\n",
        "!rm '/content/eval/ground_truth/groundtruth.tgz'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oip3Io2IzL7b"
      },
      "source": [
        "#TODO: dataset already upload\r\n",
        "import gdown\r\n",
        "!mkdir '/content/dataset/'\r\n",
        "datasetUrl = 'https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/oxbuild_images.tgz'\r\n",
        "downloadFolder = '/content/dataset/oxbuild_images.tgz' \r\n",
        "gdown.download(datasetUrl, downloadFolder, quiet=False)\r\n",
        "\r\n",
        "extract('/content/dataset/oxbuild_images.tgz', '/content/dataset')\r\n",
        "!rm '/content/dataset/oxbuild_images.tgz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FpLueRVzOVl"
      },
      "source": [
        "import numpy as np\r\n",
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "#query_file_name = \"radcliffe_camera_1_query.txt\"\r\n",
        "def cropImage(query_file_name: str, groundtruthFolderDir: str, imgFolderDir: str, outputFolderDir: str):\r\n",
        "  print(query_file_name)\r\n",
        "  query_key = query_file_name.replace(\"_query.txt\", \"\")\r\n",
        "  query_detail = load_list(groundtruthFolderDir + \"/\" + query_file_name)\r\n",
        "  list = query_detail[0].split()\r\n",
        "  imgName = list[0].replace(\"oxc1_\", \"\")\r\n",
        "  img = cv2.imread(imgFolderDir + \"/%s.jpg\" % imgName);\r\n",
        "  x1 = int(float(list[1]));\r\n",
        "  y1 = int(float(list[2]));\r\n",
        "  x2 = int(float(list[3]));\r\n",
        "  y2 = int(float(list[4]));\r\n",
        "  crop_img = img[y1:y2, x1:x2]\r\n",
        "\r\n",
        "  cv2.imwrite(outputFolderDir +\"/%s.jpg\" % query_key, crop_img)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ZJH-eYzT34"
      },
      "source": [
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "import re\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "folder = evalGroundTruth\r\n",
        "names = []\r\n",
        "for basename in os.listdir(folder):\r\n",
        "  name = os.path.splitext(basename)[0];\r\n",
        "  matches = re.findall(\".+_query\", name)\r\n",
        "  if len(matches) > 0:\r\n",
        "    names.append(name)\r\n",
        "\r\n",
        "for name in names:\r\n",
        "  fileName = name + \".txt\"\r\n",
        "  print(fileName)\r\n",
        "  cropImage(fileName, folder, datasetDir, evalQueryImg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT6hjRDDyhm0"
      },
      "source": [
        "## Run classifying query by query\r\n",
        "Browse through query_img folder and run classifying query by query. Output is relevant rank list and will be written in eval/rank_list as relevent query label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQvNwYt7HWra"
      },
      "source": [
        "# Download extracted features relevant to 5k image from Oxford\r\n",
        "# https://drive.google.com/file/d/1gl66ZCZ9zVT615hrMcoV3eaE5mweQR2i/view?usp=sharing\r\n",
        "import gdown\r\n",
        "!mkdir '/content/database/'\r\n",
        "databaseUrl = 'https://drive.google.com/uc?id=1oVi8gwA6M9WqMYBFxEuL33rn5xF_a_1u'\r\n",
        "output = '/content/database/database.zip' \r\n",
        "gdown.download(databaseUrl, output, quiet=False)\r\n",
        "\r\n",
        "!unzip -o '/content/database/database.zip' -d '/content'\r\n",
        "!rm -r '/content/database/database.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMYlPyW1QuyG"
      },
      "source": [
        "import gdown\r\n",
        "\r\n",
        "utilFilesUrl = 'https://drive.google.com/uc?id=1HTa7tTOA5KmKUpdTLNSIxBtf330pdmVV'\r\n",
        "savedDir = '/content/DeepImageUtils.py'\r\n",
        "gdown.download(utilFilesUrl, savedDir, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4djk1JMDQvYG"
      },
      "source": [
        "from shutil import copyfile\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import DeepImageUtils as IU\r\n",
        "from scipy.spatial import distance\r\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ncay3xizY-S"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "def writeToFile(dir, name, text):\r\n",
        "  with open(dir + \"/%s.txt\" % name, 'w') as f:\r\n",
        "    f.write(text)\r\n",
        "\r\n",
        "def append_multiple_lines(file_name, lines_to_append):\r\n",
        "    # Open the file in append & read mode ('a+')\r\n",
        "    with open(file_name, \"a+\") as file_object:\r\n",
        "        appendEOL = False\r\n",
        "        # Move read cursor to the start of file.\r\n",
        "        file_object.seek(0)\r\n",
        "        # Check if file is not empty\r\n",
        "        data = file_object.read(100)\r\n",
        "        if len(data) > 0:\r\n",
        "            appendEOL = True\r\n",
        "        # Iterate over each string in the list\r\n",
        "        for line in lines_to_append:\r\n",
        "            # If file is not empty then append '\\n' before first line for\r\n",
        "            # other lines always append '\\n' before appending line\r\n",
        "            if appendEOL == True:\r\n",
        "                file_object.write(\"\\n\")\r\n",
        "            else:\r\n",
        "                appendEOL = True\r\n",
        "            # Append element at the end of file\r\n",
        "            file_object.write(line)\r\n",
        "\r\n",
        "def writeRankResultToFile(dir, name, featureVectors):\r\n",
        "  text = []\r\n",
        "  for vector in featureVectors:\r\n",
        "    nameWithExt = os.path.basename(vector[0])\r\n",
        "    baseName = os.path.splitext(nameWithExt)[0]\r\n",
        "    text.append(baseName)\r\n",
        "  \r\n",
        "  filePath = dir + \"/%s.txt\" % name\r\n",
        "  append_multiple_lines(filePath, text)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWHXx2KjzZn6"
      },
      "source": [
        "folder = evalQueryImg\r\n",
        "database_path = databaseDir\r\n",
        "names = []\r\n",
        "for query_img_path in os.listdir(folder):\r\n",
        "  print(query_img_path)\r\n",
        "  name = os.path.splitext(query_img_path)[0];\r\n",
        "\r\n",
        "  query_img_categories = IU.PredictImageCategory(folder + \"/\" + query_img_path)\r\n",
        "  img_features_vector = IU.CreateImageFeaturesVector(folder + \"/\" + query_img_path)\r\n",
        "  print(query_img_categories)\r\n",
        "  print(img_features_vector)\r\n",
        "  # saving features and loading them when a query is need is slightly faster and less memory intensive than extracting the features dueing runtime\r\n",
        "  feature_vectors = []\r\n",
        "  for category in query_img_categories:\r\n",
        "      category_path = os.path.join(database_path, category + '.npz')\r\n",
        "      \r\n",
        "      if not os.path.isfile(category_path):\r\n",
        "          continue\r\n",
        "\r\n",
        "      loaded_feature_vectors = np.load(category_path, allow_pickle=True)\r\n",
        "      loaded_feature_vectors = list(loaded_feature_vectors['arr_0'])\r\n",
        "      # only add every image once (using the image name as a reference since that shouldn't be duplicated)\r\n",
        "      for loaded_feature_vector in loaded_feature_vectors:\r\n",
        "          if IU.Path2Name(loaded_feature_vector[0]) in (IU.Path2Name(feature_vector[0]) for feature_vector in feature_vectors):\r\n",
        "              continue\r\n",
        "        \r\n",
        "          feature_vectors.append(loaded_feature_vector)\r\n",
        "\r\n",
        "  feature_vectors.sort(key=lambda feature_vector: distance.cosine(feature_vector[1], img_features_vector))\r\n",
        "\r\n",
        "  writeRankResultToFile(evalRankList, name, feature_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqV22M4ByyGM"
      },
      "source": [
        "## Calculate mAP\r\n",
        "Browse through all result in rank list to calculate mAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YkU-kTYzhp3"
      },
      "source": [
        "def prepareValidationMaterial(grouthtruthUrl, rankListUrl, query):\r\n",
        "  ranked_list = load_list(rankListUrl)\r\n",
        "  pos_set = list(set(load_list(grouthtruthUrl + \"/%s_good.txt\" % query) + load_list(grouthtruthUrl + \"/%s_ok.txt\" % query)))\r\n",
        "  junk_set = load_list(grouthtruthUrl + \"/%s_junk.txt\" % query)\r\n",
        "\r\n",
        "  return ranked_list, pos_set, junk_set"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At-Bk8KWzkdC"
      },
      "source": [
        "def compute_ap(pos: List[str], amb: List[str], ranked_list: List[str]):\r\n",
        "    \"\"\"Compute average precision against a retrieved list of images. There are some bits that\r\n",
        "    could be improved in this, but is a line-to-line port of the original C++ benchmark code.\r\n",
        "    Args:\r\n",
        "        pos (List[str]): List of positive samples. This is normally a conjugation of\r\n",
        "        the good and ok samples in the ground truth data.\r\n",
        "        amb (List[str]): List of junk samples. This is normally the junk samples in\r\n",
        "        the ground truth data. Omitting this makes no difference in the AP.\r\n",
        "        ranked_list (List[str]): List of retrieved images from query to be evaluated.\r\n",
        "    Returns:\r\n",
        "        float: Average precision against ground truth - range from 0.0 (worst) to 1.0 (best).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    intersect_size, old_recall, ap = 0.0, 0.0, 0.0\r\n",
        "    old_precision, j = 1.0, 1.0\r\n",
        "\r\n",
        "    for e in ranked_list:\r\n",
        "        if e in amb:\r\n",
        "            continue\r\n",
        "\r\n",
        "        if e in pos:\r\n",
        "            intersect_size += 1.0\r\n",
        "\r\n",
        "        recall = intersect_size / len(pos)\r\n",
        "        precision = intersect_size / j\r\n",
        "        ap += (recall - old_recall) * ((old_precision + precision) / 2.0)\r\n",
        "\r\n",
        "        old_recall = recall\r\n",
        "        old_precision = precision\r\n",
        "        j += 1.0\r\n",
        "\r\n",
        "    return ap"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiPIiu1Izm9V"
      },
      "source": [
        "def compute_ap_at_k(pos: List[str], amb: List[str], ranked_list: List[str], k):\r\n",
        "    \"\"\"Compute average precision against a retrieved list of images. There are some bits that\r\n",
        "    could be improved in this, but is a line-to-line port of the original C++ benchmark code.\r\n",
        "    Args:\r\n",
        "        pos (List[str]): List of positive samples. This is normally a conjugation of\r\n",
        "        the good and ok samples in the ground truth data.\r\n",
        "        amb (List[str]): List of junk samples. This is normally the junk samples in\r\n",
        "        the ground truth data. Omitting this makes no difference in the AP.\r\n",
        "        ranked_list (List[str]): List of retrieved images from query to be evaluated.\r\n",
        "    Returns:\r\n",
        "        float: Average precision against ground truth - range from 0.0 (worst) to 1.0 (best).\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    intersect_size, old_recall, ap = 0.0, 0.0, 0.0\r\n",
        "    old_precision, j = 1.0, 1.0\r\n",
        "\r\n",
        "    for e in ranked_list:\r\n",
        "      if j <= k:\r\n",
        "        if e in amb:\r\n",
        "            continue\r\n",
        "\r\n",
        "        if e in pos:\r\n",
        "            intersect_size += 1.0\r\n",
        "\r\n",
        "        recall = intersect_size / len(pos)\r\n",
        "        precision = intersect_size / j\r\n",
        "        ap += (recall - old_recall) * ((old_precision + precision) / 2.0)\r\n",
        "\r\n",
        "        old_recall = recall\r\n",
        "        old_precision = precision\r\n",
        "        j += 1.0\r\n",
        "\r\n",
        "    return ap"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_VBeNyANPl2"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def cal_mAP():\r\n",
        "  aps = []\r\n",
        "\r\n",
        "  for query_img_path in os.listdir(evalRankList):\r\n",
        "    if query_img_path != \".ipynb_checkpoints\":\r\n",
        "      #print(query_img_path)\r\n",
        "      name = os.path.splitext(query_img_path)[0];\r\n",
        "      resultRankFileDir = evalRankList + \"/\" + name + \".txt\"\r\n",
        "      ranked_list, pos_set, junk_set = prepareValidationMaterial(evalGroundTruth, resultRankFileDir, name)\r\n",
        "\r\n",
        "      a = compute_ap(ranked_list, junk_set, pos_set)\r\n",
        "      aps.append(a);\r\n",
        "      #print(name)\r\n",
        "      #print(a);\r\n",
        "\r\n",
        "  return np.mean(aps)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R-55oVazo0X"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def cal_mAP_at_k(k):\r\n",
        "  aps = []\r\n",
        "\r\n",
        "  for query_img_path in os.listdir(evalRankList):\r\n",
        "    if query_img_path != \".ipynb_checkpoints\":\r\n",
        "      #print(query_img_path)\r\n",
        "      name = os.path.splitext(query_img_path)[0];\r\n",
        "      resultRankFileDir = evalRankList + \"/\" + name + \".txt\"\r\n",
        "      ranked_list, pos_set, junk_set = prepareValidationMaterial(evalGroundTruth, resultRankFileDir, name)\r\n",
        "\r\n",
        "      a = compute_ap_at_k(ranked_list, junk_set, pos_set, k)\r\n",
        "      aps.append(a);\r\n",
        "      #print(name)\r\n",
        "      #print(a);\r\n",
        "      \r\n",
        "  return np.mean(aps)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNCoOqYHzqx3",
        "outputId": "16c32ea3-d05e-4f1c-8e05-72dd25e36864"
      },
      "source": [
        "mAP1 = cal_mAP_at_k(1)\r\n",
        "mAP4 = cal_mAP_at_k(4)\r\n",
        "mAP10 = cal_mAP_at_k(10)\r\n",
        "mAP100 = cal_mAP_at_k(100)\r\n",
        "mAP = cal_mAP()\r\n",
        "\r\n",
        "print(\"mAP@1: \\t\\t %s\" % mAP1)\r\n",
        "print(\"mAP@4: \\t\\t %s\" % mAP4)\r\n",
        "print(\"mAP@10: \\t %s\" % mAP10)\r\n",
        "print(\"mAP@100: \\t %s\" % mAP100)\r\n",
        "print(\"mAP: \\t\\t %s\" % mAP)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mAP@1: \t\t 0.0003649085211765999\n",
            "mAP@4: \t\t 0.0014558023458471584\n",
            "mAP@10: \t 0.003504431619723861\n",
            "mAP@100: \t 0.015952104736482512\n",
            "mAP: \t\t 0.02045981028407575\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}